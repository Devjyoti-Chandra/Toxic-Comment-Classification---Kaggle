{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import string\n",
    "import re\n",
    "import xgboost as xgb\n",
    "import gc\n",
    "from sklearn import preprocessing, metrics, ensemble, neighbors, linear_model, tree, model_selection\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import manifold, decomposition, naive_bayes\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm as tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from subprocess import check_output\n",
    "#print(check_output([\"ls\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = train_df.fillna(\"unknown\")\n",
    "test_df = test_df.fillna(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rows in train dataset : \",train_df.shape[0])\n",
    "print(\"Number of rows in test dataset : \",test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Distribution of toxic : \\n\", train_df.toxic.value_counts())\n",
    "print(\"Distribution of severe_toxic : \\n\", train_df.severe_toxic.value_counts())\n",
    "print(\"Distribution of obscene : \\n\", train_df.obscene.value_counts())\n",
    "print(\"Distribution of threat : \\n\", train_df.threat.value_counts())\n",
    "print(\"Distribution of insult : \\n\", train_df.insult.value_counts())\n",
    "print(\"Distribution of toxic : \\n\", train_df.identity_hate.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#l=train.toxic+train.severe_toxic+train.obscene+train.threat+train.insult+train.identity_hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#l.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['comment_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean the comment\n",
    "# adapted from a kaggle kernal, can't find its link now\n",
    "def cleanData(text):\n",
    "    \n",
    "    \n",
    "    text = re.sub(r\"\\n\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"\", text)\n",
    "    text = re.sub(r\"What's\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"I'm\", \"I am\", text)\n",
    "    text = re.sub(r\" m \", \" am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\"60k\", \" 60000 \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e-mail\", \"email\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = re.sub(r\"quikly\", \"quickly\", text)\n",
    "    text = re.sub(r\" usa \", \" America \", text)\n",
    "    text = re.sub(r\" USA \", \" America \", text)\n",
    "    text = re.sub(r\" u s \", \" America \", text)\n",
    "    text = re.sub(r\" uk \", \" England \", text)\n",
    "    text = re.sub(r\" UK \", \" England \", text)\n",
    "    text = re.sub(r\"india\", \"India\", text)\n",
    "    text = re.sub(r\" dms \", \"direct messages \", text)  \n",
    "    text = re.sub(r\"actived\", \"active\", text)\n",
    "    text = re.sub(r\"kms\", \" kilometers \", text)\n",
    "    text = re.sub(r\"KMs\", \" kilometers \", text)\n",
    "    text = re.sub(r\"\\0rs \", \" rs \", text) \n",
    "    text = re.sub(r\"calender\", \"calendar\", text)\n",
    "    text = re.sub(r\"ios\", \"operating system\", text)\n",
    "    text = re.sub(r\"bestfriend\", \"best friend\", text)\n",
    "    text = re.sub(r\"dna\", \"DNA\", text)\n",
    "    text = re.sub(r\"III\", \"3\", text) \n",
    "    text = re.sub(r\"Find\", \"find\", text) \n",
    "    text = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['comment_text'] = train_df['comment_text'].apply(lambda x: cleanData(x))\n",
    "test_df['comment_text'] = test_df['comment_text'].apply(lambda x: cleanData(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function for stemming and lemmatizing the text\n",
    "def cleanData2(text, stemming = False, lemmatize=False):\n",
    "    txt = str(text)\n",
    "    txt = re.sub(r'-', r'', txt)\n",
    "    \n",
    "   \n",
    "    if stemming:\n",
    "        st = EnglishStemmer()\n",
    "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "\n",
    "    \n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w) for w in txt.split()])\n",
    "        \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df['comment_text'] = train_df['comment_text'].map(lambda x: cleanData2(x,  stemming = True, lemmatize=True))\n",
    "test_df['comment_text'] = test_df['comment_text'].map(lambda x: cleanData2(x,  stemming = True, lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('data/train_cleaned.csv', index=False)\n",
    "test_df.to_csv('data/test_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_word = TfidfVectorizer()\n",
    "X_tfidf_word = tfidf_word.fit_transform(train_df['comment_text'])\n",
    "tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(1, 2), lowercase=False)\n",
    "X_tfidf_char = tfidf_char.fit_transform(train_df['comment_text'])\n",
    "X_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_word = TfidfVectorizer()\n",
    "X1_tfidf_word = tfidf_word.fit_transform(test_df['comment_text'])\n",
    "tfidf_char = TfidfVectorizer(analyzer='char', ngram_range=(1, 2), lowercase=False)\n",
    "X1_tfidf_char = tfidf_char.fit_transform(test_df['comment_text'])\n",
    "X1_tfidf = sparse.hstack([X_tfidf_word, X_tfidf_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the tfidf vectorizer ###\n",
    "tfidf_vec = TfidfVectorizer(analyzer='char',stop_words='english', max_features=8000,, ngram_range=(1,3))\n",
    "full_tfidf = tfidf_vec.fit_transform(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_com = TfidfVectorizer(stop_words='english', max_features=8000, ngram_range=(1,3)).fit(pd.concat([train['comment_text'],test['comment_text']],axis=0))\n",
    "comments_train = transform_com.transform(train['comment_text'])\n",
    "comments_test = transform_com.transform(test['comment_text'])\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_comp = 80\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(full_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "train_df = pd.concat([train_df, train_svd], axis=1)\n",
    "test_df = pd.concat([test_df, test_svd], axis=1)\n",
    "del full_tfidf, train_tfidf, test_tfidf, train_svd, test_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fit transform the count vectorizer ###\n",
    "tfidf_vec = CountVectorizer(stop_words='english', max_features=5000, ngram_range=(1,2))\n",
    "tfidf_vec.fit(train_df['comment_text'].values.tolist() + test_df['comment_text'].values.tolist())\n",
    "train_tfidf = tfidf_vec.transform(train_df['comment_text'].values.tolist())\n",
    "test_tfidf = tfidf_vec.transform(test_df['comment_text'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, feature_names=None, seed_val=0, rounds=5000, dep=6, eta=0.1):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params['eval_metric'] = 'logloss'\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"subsample\"] = 0.6\n",
    "    params[\"min_child_weight\"] = 5\n",
    "    params[\"colsample_bytree\"] = 0.6\n",
    "    params[\"max_depth\"] = dep\n",
    "\n",
    "    params[\"silent\"] = 1\n",
    "    params[\"seed\"] = seed_val\n",
    "    #params[\"max_delta_step\"] = 2\n",
    "    #params[\"gamma\"] = 0.5\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    if feature_names is not None:\n",
    "        create_feature_map(feature_names)\n",
    "        model.dump_model('xgbmodel.txt', 'xgb.fmap', with_stats=True)\n",
    "        importance = model.get_fscore(fmap='xgb.fmap')\n",
    "        importance = sorted(importance.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        imp_df = pd.DataFrame(importance, columns=['feature','fscore'])\n",
    "        imp_df['fscore'] = imp_df['fscore'] / imp_df['fscore'].sum()\n",
    "        imp_df.to_csv(\"imp_feat.txt\", index=False)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit=model.best_ntree_limit)\n",
    "    pred_test_y2 = model.predict(xgb.DMatrix(test_X2), ntree_limit=model.best_ntree_limit)\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.log_loss(test_y, pred_test_y)\n",
    "        return pred_test_y, loss, pred_test_y2\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "preds = np.zeros((test_df.shape[0], len(col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols_to_drop = ['id', 'comment_text']\n",
    "train_X = train_df.drop(cols_to_drop+col, axis=1)\n",
    "test_X = test_df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in [\"XGB1\"]:\n",
    "    avg_score = 0\n",
    "    for i, j in enumerate(col):\n",
    "    \n",
    "\n",
    "        print(\"Model building..\")\n",
    "        kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        cv_scores = []\n",
    "        \n",
    "        pred_test_full = 0\n",
    "        pred_val_full = np.zeros(train_df.shape[0])\n",
    "        for dev_index, val_index in kf.split(train_tfidf, train_df[j]):\n",
    "            dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "            dev_y, val_y = train_df[j][dev_index], train_df[j][val_index]\n",
    "\n",
    "            if model_name == \"XGB1\":\n",
    "                pred_val, loss, pred_test = runXGB(dev_X, dev_y, val_X, val_y, test_tfidf, rounds=5000)#, feature_names=dev_X.columns.tolist())\n",
    "            elif model_name == \"LGB1\":\n",
    "                pred_val, loss, pred_test = runLGB(dev_X, dev_y, val_X, val_y, test_tfidf, rounds=5000)\n",
    "            pred_val_full[val_index] = pred_val\n",
    "            pred_test_full = pred_test_full + pred_test\n",
    "            cv_scores.append(loss)\n",
    "            print(cv_scores)\n",
    "        pred_test_full /= 5.\n",
    "        preds[:,i] = pred_test_full\n",
    "        avg_score = avg_score + metrics.log_loss(train_df[j], pred_val_full)\n",
    "        print(metrics.log_loss(train_df[j], pred_val_full))\n",
    "    print(avg_score/6)\n",
    "        #out_df = pd.DataFrame({\"transaction_id\":test_id})\n",
    "        #out_df[\"target\"] = pred_test_full\n",
    "        #out_df.to_csv(\"pred_test_v5_\"+model_name+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1500\n",
    "0.174851923545  0.0307588272292  0.097675168881  0.0134371405698  0.110697548472  0.0331138548092 #sub1\n",
    "0.172551708056  0.0303350523081  0.096968593905  0.0131761250039  0.109821485807  0.0327623291308 #sub2\n",
    "0.171068898419  0.0302236932618  0.096428283268  0.0131215505595  0.109139308988  0.0324764278088 #sub3\n",
    "0.170988788114  0.0302762238162  0.096480449315  0.0131473827986  0.108874484126  0.0324891823683 #sub4\n",
    "0.170047287624  0.0303037070276  0.096302944380  0.0130279706331  0.108270451596  0.0324051076683 #xgb(6-0.1-0.6-5-0.6).csv\n",
    "\n",
    "0.132674747535  0.0283703857325  0.073061890770  0.0116233136495  0.085781432600  0.0298326837564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subm = pd.read_csv('data/sample_submission.csv')    \n",
    "submid = pd.DataFrame({'id': subm[\"id\"]})\n",
    "submission = pd.concat([submid, pd.DataFrame(preds, columns = col)], axis=1)\n",
    "submission.to_csv('xgb_stemmed_lemmatized(6-0.1-0.6-5-0.6).csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runMNB(train_X, train_y, test_X, test_y, test_X2):\n",
    "    model = naive_bayes.MultinomialNB()\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict(test_X)\n",
    "    pred_test_y2 = model.predict(test_X2)\n",
    "    return pred_test_y, pred_test_y2, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_score = 0\n",
    "for i, j in enumerate(col):\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros(train_df.shape[0])\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    for dev_index, val_index in kf.split(train_tfidf, train_df[j]):\n",
    "        dev_X, val_X = train_tfidf[dev_index], train_tfidf[val_index]\n",
    "        dev_y, val_y = train_df[j][dev_index], train_df[j][val_index]\n",
    "        pred_val_y, pred_test_y, model = runMNB(dev_X, dev_y, val_X, val_y, test_tfidf)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        cv_scores.append(metrics.log_loss(val_y, pred_val_y))\n",
    "        print(cv_scores)\n",
    "    print(\"Mean cv score : \", np.mean(cv_scores))\n",
    "    pred_full_test = pred_full_test / 5.\n",
    "    avg_score = avg_score + metrics.log_loss(train_df[j], pred_train)\n",
    "print(avg_score/6)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
